# CNM-BERT Pre-training Configuration
# Base configuration for 8x H100 80GB GPUs

# Random seed for reproducibility
seed: 42

# Model configuration
model:
  init_from_bert: true  # Initialize from bert-base-chinese
  bert_model_name: "bert-base-chinese"
  tree_path: "data/processed/char_to_ids_tree.json"
  vocab_size: 21128
  hidden_size: 768
  num_hidden_layers: 12
  num_attention_heads: 12
  intermediate_size: 3072
  struct_dim: 256
  fusion_strategy: "concat"  # concat, add, or gate
  freeze_bert_encoder: false  # Set to true for curriculum learning

# Tokenizer configuration
tokenizer:
  vocab_path: "bert-base-chinese"
  struct_path: "data/processed/char_to_ids_tree.json"

# Data configuration
data:
  train_file: "data/pretrain/corpus_clean.txt"
  val_file: null  # Optional validation file
  max_train_samples: null  # null = use all data
  max_val_samples: null

# Data collator configuration
collator:
  mlm_probability: 0.15
  max_seq_length: 512

# Training configuration
training:
  output_dir: "experiments/logs/cnm_bert_base"
  num_train_epochs: 3
  max_steps: 1000000  # Override epochs if set
  per_device_train_batch_size: 32  # Per GPU batch size
  per_device_eval_batch_size: 64
  gradient_accumulation_steps: 1
  learning_rate: 1.0e-4
  weight_decay: 0.01
  warmup_steps: 10000
  logging_steps: 100
  save_steps: 10000
  save_total_limit: 3
  eval_steps: 50000
  fp16: true
  num_workers: 4
  use_wandb: true
  run_name: "cnm-bert-base-pretrain"

# Notes:
# - Global batch size = per_device_train_batch_size * gradient_accumulation_steps * num_gpus
# - For 8 GPUs: 32 * 1 * 8 = 256 global batch size
# - Adjust learning_rate and warmup_steps based on batch size
